{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playing with neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms import RandomApply\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "xce = nn.CrossEntropyLoss()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up a Convolution Neural Network (CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, in_channels, output_dim):\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        self.cnn1 = nn.Conv2d(in_channels=in_channels, out_channels=32, kernel_size=5)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "        self.cnn2 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=5)\n",
    "        self.relu2 = nn.ReLU()\n",
    "\n",
    "        self.cnn3 = nn.Conv2d(in_channels=in_channels,out_channels=64, kernel_size=5 )\n",
    "        self.relu3 = nn.ReLU()\n",
    "        # Adding 2 additional conv layers \n",
    "        \"\"\"         self.cnn3 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=5)\n",
    "        self.relu3 = nn.ReLU()\n",
    "\n",
    "        self.cnn4 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=5)\n",
    "        self.relu4 = nn.ReLU() \"\"\"\n",
    "        # Add more convolutional layers if needed\n",
    "\n",
    "\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "        if in_channels == 1:\n",
    "            input_dim = 32 * 4 * 4\n",
    "        else:\n",
    "            input_dim = 32 * 5 * 5\n",
    "\n",
    "        self.fc1 = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.cnn1(x)\n",
    "        out = self.relu1(out)\n",
    "\n",
    "        out = self.maxpool1(out)\n",
    "\n",
    "        out = self.cnn2(out)\n",
    "        out = self.relu2(out)\n",
    "\n",
    "        out = self.maxpool2(out)\n",
    "\n",
    "        out = out.view(out.size(0), -1)\n",
    "\n",
    "        out = self.fc1(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (cnn1): Conv2d(3, 32, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (relu1): ReLU()\n",
      "  (maxpool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (cnn2): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (relu2): ReLU()\n",
      "  (cnn3): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (relu3): ReLU()\n",
      "  (maxpool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=800, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "output_dim = 10\n",
    "model = CNN(3, output_dim)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "batch_size = 100\n",
    "iterations = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' transform = transforms.Compose([\\n    transforms.ToTensor(),\\n    transforms.RandomHorizontalFlip(),\\n    transforms.RandomRotation(10),\\n]) '"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "transform = transforms.Compose([transforms.ToTensor(),])\n",
    "#test to see if transform works\n",
    "\"\"\" transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "]) \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import repeat\n",
    "def inf_loop(data_loader):\n",
    "    for loader in repeat(data_loader):\n",
    "        yield from loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "test_mnist_set  = datasets.MNIST(root=\"./data\", train=False, download=True, transform=transform)\n",
    "train_mnist_set = datasets.MNIST(root=\"./data\", train=True,  download=True, transform=transform)\n",
    "\n",
    "train_cifar10_set = datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=transform)\n",
    "test_cifar10_set = datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=transform)\n",
    "\n",
    "noisy_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    RandomApply([transforms.GaussianBlur(kernel_size=3)], p=0.5),\n",
    "    # Add more transformations if needed\n",
    "])\n",
    "\n",
    "train_noisy_cifar10_set = datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=noisy_transform)\n",
    "test_noisy_cifar10_set = datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=noisy_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_cifar10_loader = inf_loop(DataLoader(train_cifar10_set, batch_size = batch_size, shuffle = False))\n",
    "test_cifar10_loader  =          DataLoader(test_cifar10_set,  batch_size = batch_size, shuffle = False)\n",
    "\n",
    "train_noisy_cifar10_loader = inf_loop(DataLoader(train_noisy_cifar10_set, batch_size = batch_size, shuffle = False))\n",
    "test_noisy_cifar10_loader = DataLoader(test_noisy_cifar10_set, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:   100  Loss: 2.1325  Accuracy regular: 25.63%\n",
      "Iteration:   100  Loss: 2.1325  Accuracy noisy: 25.47%\n",
      "Iteration:   200  Loss: 1.8787  Accuracy regular: 30.83%\n",
      "Iteration:   200  Loss: 1.8787  Accuracy noisy: 29.66%\n",
      "Iteration:   300  Loss: 1.8030  Accuracy regular: 29.81%\n",
      "Iteration:   300  Loss: 1.8030  Accuracy noisy: 31.11%\n",
      "Iteration:   400  Loss: 1.6654  Accuracy regular: 40.95%\n",
      "Iteration:   400  Loss: 1.6654  Accuracy noisy: 40.94%\n",
      "Iteration:   500  Loss: 1.7237  Accuracy regular: 40.11%\n",
      "Iteration:   500  Loss: 1.7237  Accuracy noisy: 40.74%\n",
      "Iteration:   600  Loss: 1.5365  Accuracy regular: 46.66%\n",
      "Iteration:   600  Loss: 1.5365  Accuracy noisy: 46.26%\n",
      "Iteration:   700  Loss: 1.4599  Accuracy regular: 49.00%\n",
      "Iteration:   700  Loss: 1.4599  Accuracy noisy: 48.64%\n",
      "Iteration:   800  Loss: 1.2653  Accuracy regular: 49.37%\n",
      "Iteration:   800  Loss: 1.2653  Accuracy noisy: 49.12%\n",
      "Iteration:   900  Loss: 1.3168  Accuracy regular: 50.80%\n",
      "Iteration:   900  Loss: 1.3168  Accuracy noisy: 50.33%\n",
      "Iteration:  1000  Loss: 1.5400  Accuracy regular: 49.07%\n",
      "Iteration:  1000  Loss: 1.5400  Accuracy noisy: 48.83%\n",
      "Iteration:  1100  Loss: 1.3898  Accuracy regular: 52.54%\n",
      "Iteration:  1100  Loss: 1.3898  Accuracy noisy: 52.02%\n",
      "Iteration:  1200  Loss: 1.3362  Accuracy regular: 53.75%\n",
      "Iteration:  1200  Loss: 1.3362  Accuracy noisy: 52.81%\n",
      "Iteration:  1300  Loss: 1.1504  Accuracy regular: 53.53%\n",
      "Iteration:  1300  Loss: 1.1504  Accuracy noisy: 52.46%\n",
      "Iteration:  1400  Loss: 1.1697  Accuracy regular: 53.11%\n",
      "Iteration:  1400  Loss: 1.1697  Accuracy noisy: 53.02%\n",
      "Iteration:  1500  Loss: 1.3442  Accuracy regular: 55.10%\n",
      "Iteration:  1500  Loss: 1.3442  Accuracy noisy: 54.73%\n",
      "Iteration:  1600  Loss: 1.2399  Accuracy regular: 55.38%\n",
      "Iteration:  1600  Loss: 1.2399  Accuracy noisy: 54.01%\n",
      "Iteration:  1700  Loss: 1.2481  Accuracy regular: 57.04%\n",
      "Iteration:  1700  Loss: 1.2481  Accuracy noisy: 56.38%\n",
      "Iteration:  1800  Loss: 1.0326  Accuracy regular: 55.72%\n",
      "Iteration:  1800  Loss: 1.0326  Accuracy noisy: 54.19%\n",
      "Iteration:  1900  Loss: 1.0898  Accuracy regular: 55.78%\n",
      "Iteration:  1900  Loss: 1.0898  Accuracy noisy: 55.18%\n",
      "Iteration:  2000  Loss: 1.2162  Accuracy regular: 57.56%\n",
      "Iteration:  2000  Loss: 1.2162  Accuracy noisy: 56.87%\n"
     ]
    }
   ],
   "source": [
    "loss_list = []\n",
    "iteration_list = []\n",
    "accuracy_list = []\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "for i, (images, labels) in enumerate(train_cifar10_loader, 1):\n",
    "\n",
    "    images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "    train  = images\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    outputs = model(train)\n",
    "\n",
    "    loss = xce(outputs, labels)\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    if i % 100 == 0:\n",
    "\n",
    "        # Regular CIFAR-10 evaluation\n",
    "        correct_regular = 0\n",
    "        total_regular = 0\n",
    "\n",
    "        for images, labels in test_cifar10_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            predicted = torch.argmax(outputs, dim=1)\n",
    "            total_regular += labels.size(0)\n",
    "            correct_regular += (predicted == labels).sum().item()\n",
    "\n",
    "        accuracy_regular = 100 * correct_regular / total_regular\n",
    "        print('Iteration: {:5d}  Loss: {:.4f}  Accuracy regular: {:.2f}%'.format(i, loss.data, accuracy_regular))\n",
    "\n",
    "\n",
    "        # Noisy CIFAR-10 evaluation\n",
    "        correct_noisy = 0\n",
    "        total_noisy = 0\n",
    "\n",
    "        for images, labels in test_noisy_cifar10_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            predicted = torch.argmax(outputs, dim=1)\n",
    "            total_noisy += labels.size(0)\n",
    "            correct_noisy += (predicted == labels).sum().item()\n",
    "\n",
    "        accuracy_noisy = 100 * correct_noisy / total_noisy\n",
    "        print('Iteration: {:5d}  Loss: {:.4f}  Accuracy noisy: {:.2f}%'.format(i, loss.data, accuracy_noisy))\n",
    "\n",
    "\n",
    "    if i == iterations:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:   100  Loss: 2.3110  Accuracy: 10.02%\n",
      "Iteration:   200  Loss: 2.3017  Accuracy: 10.06%\n",
      "Iteration:   300  Loss: 2.3097  Accuracy: 10.05%\n",
      "Iteration:   400  Loss: 2.3068  Accuracy: 10.05%\n",
      "Iteration:   500  Loss: 2.3191  Accuracy: 10.04%\n",
      "Iteration:   600  Loss: 2.3106  Accuracy: 10.05%\n",
      "Iteration:   700  Loss: 2.3018  Accuracy: 10.06%\n",
      "Iteration:   800  Loss: 2.3093  Accuracy: 10.05%\n",
      "Iteration:   900  Loss: 2.3074  Accuracy: 10.04%\n",
      "Iteration:  1000  Loss: 2.3193  Accuracy: 10.05%\n",
      "Iteration:  1100  Loss: 2.3110  Accuracy: 10.06%\n",
      "Iteration:  1200  Loss: 2.3019  Accuracy: 10.05%\n",
      "Iteration:  1300  Loss: 2.3103  Accuracy: 10.05%\n",
      "Iteration:  1400  Loss: 2.3071  Accuracy: 10.06%\n",
      "Iteration:  1500  Loss: 2.3196  Accuracy: 10.05%\n",
      "Iteration:  1600  Loss: 2.3111  Accuracy: 10.04%\n",
      "Iteration:  1700  Loss: 2.3022  Accuracy: 10.06%\n",
      "Iteration:  1800  Loss: 2.3098  Accuracy: 10.06%\n",
      "Iteration:  1900  Loss: 2.3066  Accuracy: 10.04%\n",
      "Iteration:  2000  Loss: 2.3196  Accuracy: 10.05%\n"
     ]
    }
   ],
   "source": [
    "loss_list = []\n",
    "iteration_list = []\n",
    "accuracy_list = []\n",
    "\n",
    "\n",
    "model2 = CNN(3, output_dim)\n",
    "model2 = model2.to(device)\n",
    "\n",
    "for i, (images, labels) in enumerate(train_noisy_cifar10_loader, 1):\n",
    "\n",
    "    images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "    train  = images\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    outputs = model2(train)\n",
    "\n",
    "    loss = xce(outputs, labels)\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    if i % 100 == 0:\n",
    "\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for images, labels in test_noisy_cifar10_loader:\n",
    "\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            test = images\n",
    "            outputs = model2(test)\n",
    "\n",
    "            # Get predictions from the maximum value\n",
    "            predicted = torch.max(outputs.data, 1)[1]\n",
    "\n",
    "            total += len(labels)\n",
    "            correct += (predicted == labels).sum()\n",
    "\n",
    "        accuracy = 100 * correct / float(total)\n",
    "\n",
    "        loss_list.append(loss.data)\n",
    "        iteration_list.append(i)\n",
    "        accuracy_list.append(accuracy)\n",
    "        print('Iteration: {:5d}  Loss: {:.4f}  Accuracy: {:.2f}%'.format(i, loss.data, accuracy))\n",
    "\n",
    "    if i == iterations:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
